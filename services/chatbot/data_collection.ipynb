{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリーのインストール\n",
    "pip install tqdm PyMuPDF pdf2image requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリーのインポート\n",
    "import os\n",
    "import requests\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pdf links の取得\n",
    "def download_pdf_links(save_path: str, folder_name: str) -> None:\n",
    "    urls = set()\n",
    "    with open('links.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            urls.add(line.strip())\n",
    "\n",
    "    for url in urls:\n",
    "        download_pdf_from_url(url, save_path, folder_name)\n",
    "\n",
    "# pdf のダウンロード\n",
    "def download_pdf_from_url(url: str, save_path: str, folder_name: str) -> None:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "\n",
    "            # Handle relative paths\n",
    "            if not href.startswith('http'):\n",
    "                href = urllib.parse.urljoin(url, href)\n",
    "\n",
    "            if href.endswith(\".pdf\"):\n",
    "                download_pdf(href, save_path, folder_name)\n",
    "\n",
    "def download_pdf(pdf_link: str, save_path: str, folder_name: str) -> None:\n",
    "    response = requests.get(pdf_link)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        filename = os.path.basename(urllib.parse.urlsplit(pdf_link).path)\n",
    "        folder_path = os.path.join(save_path, folder_name)\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        print(f\"Downloaded {filename} successfully.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to download {pdf_link}.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    download_pdf_links(save_path=\"\", folder_name=\"pdf_files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pdf からテキストを抽出\n",
    "\n",
    "pdf_directory = 'pdf_files'\n",
    "output_file = 'data.txt'\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    text = ''\n",
    "    with fitz.open(pdf_file) as doc:\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "    return text.strip()\n",
    "\n",
    "def main():\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('')\n",
    "\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            print(f\"Processing: {pdf_path}\")\n",
    "            \n",
    "            pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            with open(output_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(f\"PDF: {filename}\\n\")\n",
    "                f.write(pdf_text + '\\n\\n')\n",
    "\n",
    "    print(f\"All PDFs processed. Text saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data のクリーニング\n",
    "\n",
    "import re\n",
    "\n",
    "input_file = 'data.txt'\n",
    "output_file = 'cleaned_data.txt'\n",
    "\n",
    "def clean_data(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    cleaned_data = re.sub(r'[^\\u3041-\\u3096\\u30A1-\\u30FA\\u4E00-\\u9FFF\\s]', '', data)\n",
    "\n",
    "    cleaned_data = re.sub(r'\\s+', ' ', cleaned_data)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(cleaned_data.strip())\n",
    "\n",
    "    print(f\"Cleaned data saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_data(input_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
